{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\bohra\\\\Mnist_fashion_model\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\bohra\\\\Mnist_fashion_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_optimizer: int\n",
    "    params_learning_rate: float\n",
    "    param: dict\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = (self.config.data_ingestion.train_data)\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            base_model_path=Path(prepare_base_model.base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.num_epochs,\n",
    "            params_batch_size=params.batch_size,\n",
    "            params_optimizer=params.optimizer,\n",
    "            params_learning_rate = params.learning_rate,\n",
    "            param = dict(params),\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from cnnClassifier import logger\n",
    "from cnnClassifier.utils.training_utils import log_model_n_params,validate_model\n",
    "from cnnClassifier.utils.training_utils import evaluation_metrics_n_Hyperparameters\n",
    "from cnnClassifier.utils.common import transform_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "\n",
    "    def _init_(self):\n",
    "        super(FashionCNN, self)._init_()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "model = FashionCNN()\n",
    "\n",
    "# Define the transform_train function\n",
    "def transform_train():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "class Training:\n",
    "\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_loss_list = []\n",
    "        self.train_accuracy_list = []\n",
    "        self.val_loss_list = [] \n",
    "        self.val_accuracy_list = []\n",
    "        self.best_val_accuracy = 0\n",
    "        self.model = FashionCNN()\n",
    "        logger.info(self.model )\n",
    "\n",
    "    def train_val_split(self):\n",
    "        train_set =torchvision.datasets.FashionMNIST(self.config.training_data, download=True,transform=transform_train())\n",
    "\n",
    "\n",
    "        train_ratio = 0.8  # Adjust as needed\n",
    "        train_size = int(train_ratio * len(train_set))\n",
    "        val_size = len(train_set) - train_size\n",
    "\n",
    "        train_dataset, val_dataset = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=self.config.params_batch_size)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=self.config.params_batch_size)\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        # self.model = torch.load(f'{self.config.base_model_path}', map_location=self.device)\n",
    "        self.model = FashionCNN()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model, self.config.trained_model_path)\n",
    "    \n",
    "    def train_model(self):\n",
    "\n",
    "        error = nn.CrossEntropyLoss()\n",
    "        model1 = self.model\n",
    "        # Initialize the optimizer based on optimizer_name\n",
    "        if self.config.params_optimizer == 0:\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        elif self.config.params_optimizer == 1:\n",
    "            optimizer = torch.optim.SGD(self.model1.parameters(), lr=self.config.params_learning_rate)\n",
    "\n",
    "        # Lists for knowing classwise accuracy\n",
    "        predictions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        scheduler = StepLR(optimizer, step_size=(self.config.params_epochs)/2, gamma=0.1)\n",
    "       \n",
    "        # Directory to save model checkpoints\n",
    "        checkpoint_dir = 'checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        file_path = 'checkpoints\\checkpoint_model.pth'\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            self.model = torch.load('checkpoints\\checkpoint_model.pth', map_location=self.device)\n",
    "            logger.info('checkpoint_model')\n",
    "\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "\n",
    "            for images, labels in self.train_loader:\n",
    "\n",
    "                # Transfering images and labels to GPU if available\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # if images.shape[1] == 1:\n",
    "                #  images = images.repeat(1, 3, 1, 1)  # Repeat the grayscale channel 3 times\n",
    "\n",
    "                train = images.view(100, 1, 28, 28)\n",
    "                labels = labels\n",
    "\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(train)\n",
    "                loss = error(outputs, labels)\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                predictions = torch.max(outputs, 1)[1].to(self.device)\n",
    "                predictions_list.append(predictions)\n",
    "                labels_list.append(labels)\n",
    "\n",
    "                # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "                optimizer.zero_grad()\n",
    "                #Propagating the error backward\n",
    "                loss.backward()\n",
    "                # Optimizing the parameters\n",
    "                optimizer.step()\n",
    "                # Total\n",
    "                total += len(labels)\n",
    "\n",
    "            # loss calculation\n",
    "            train_loss = running_loss / total\n",
    "\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "        # validation\n",
    "            validate_model(self.model,self.valid_loader,checkpoint_dir,epoch,self.best_val_accuracy,\n",
    "                                 self.device,self.val_loss_list,self.val_accuracy_list)\n",
    "        #logging and evaluating metrics_n_Hyperparameters\n",
    "            evaluation_metrics_n_Hyperparameters(self,labels_list,predictions_list,train_loss,epoch)\n",
    "\n",
    "        #plotting loss and accuracy\n",
    "        plot_loss_accuracy(self.train_loss_list, self.train_accuracy_list, \n",
    "                           self.val_loss_list, self.val_accuracy_list,self.config.params_epochs)\n",
    "\n",
    "        #logging the model\n",
    "        log_model_n_params(self.model,labels_list,predictions_list,self.config.param)\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 17:55:32,748: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-26 17:55:32,752: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-26 17:55:32,754: INFO: common: created directory at: artifacts]\n",
      "[2024-07-26 17:55:32,755: INFO: common: created directory at: artifacts\\training]\n",
      "[2024-07-26 17:55:32,756: INFO: 432966609: FashionCNN()]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     training\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[0;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain_val_split()\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     training\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[10], line 88\u001b[0m, in \u001b[0;36mTraining.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Initialize the optimizer based on optimizer_name\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mparams_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_learning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mparams_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     90\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel1\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mparams_learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\bohra\\anaconda3\\envs\\honey_mnist\\lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\bohra\\anaconda3\\envs\\honey_mnist\\lib\\site-packages\\torch\\optim\\optimizer.py:279\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    277\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    281\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_val_split()\n",
    "    training.train_model()\n",
    "    training.save_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
